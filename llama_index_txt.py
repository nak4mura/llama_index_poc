from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    get_response_synthesizer,
    Settings
)
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.query_engine import RetrieverQueryEngine
from llm_settings import set_llm, get_embed_model
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.response_synthesizers import ResponseMode
from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler, CBEventType
import logging
import sys
logging.basicConfig(
    level=logging.DEBUG,
    force=True,
    format="%(asctime)s %(levelname)s %(name)s %(message)s",
    handlers=[
        logging.FileHandler("tmp.log")
    ]
)

class Tee:
    def __init__(self, *files):
        self.files = files
    def write(self, data):
        for f in self.files:
            f.write(data)
            f.flush()
    def flush(self):
        for f in self.files:
            f.flush()

log_file = open("output.log", "w")
sys.stdout = Tee(sys.__stdout__, log_file)
sys.stderr = Tee(sys.__stderr__, log_file)

set_llm()
embed_model = get_embed_model()


# 128, 256, 512, 1024, 2048
Settings.chunk_size=512

# ãƒ‡ãƒãƒƒã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®è¨­å®š
debug_handler = LlamaDebugHandler(print_trace_on_end=True)
callback_manager = CallbackManager([debug_handler])
# Settings ã« LLM ã¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’è¨­å®š
Settings.callback_manager = callback_manager

# Documentsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆ
documents = SimpleDirectoryReader(
    input_dir="./input/txt/",
).load_data()

# Indexã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆ
index = VectorStoreIndex.from_documents(
    documents,
    show_progress=True
)

# Retriever + SimpleChatEngine æ§‹ç¯‰
retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=10,
    embed_model=embed_model
)

response_synthesizer = get_response_synthesizer(
    # Response Mode	æ¦‚è¦
    # ResponseMode.REFINE	å„ãƒãƒ¼ãƒ‰ã‚’é †æ¬¡èª¿ã¹ã¦ç­”ãˆã‚’ä½œæˆã—ãªãŒã‚‰æ´—ç·´ã•ã›ã¦ã„ã
    # ResponseMode.COMPACT	å®Ÿéš›ã¯CompactAndRefineã¨ã„ã†å‡¦ç†ã§ã‚ã‚Šã€ã»ã¼Refineã¨åŒã˜ã ãŒã€ãƒãƒ£ãƒ³ã‚¯ã‚’LLMã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æœ€å¤§é™åˆ©ç”¨ã™ã‚‹ã‚ˆã†ã«repackã™ã‚‹ãŸã‚ã‚ˆã‚Šé«˜é€Ÿ
    # ResponseMode.TREE_SUMMARIZE	æŠ½å‡ºã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã‚’ä½¿ã„ã‚µãƒãƒ©ã‚¤ã‚ºã‚’ç¹°ã‚Šè¿”ã™ã“ã¨ã§ãƒãƒ£ãƒ³ã‚¯ã‚’åœ§ç¸®ã—ãŸå¾Œã«ã‚¯ã‚¨ãƒªã‚’è¡Œã†ã€‚ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã‚Šå¤šå°‘å‹•ä½œãŒå¤‰ã‚ã‚‹
    # ResponseMode.SIMPLE_SUMMARIZE	ãƒãƒ¼ãƒ‰ã‚’å˜ç´”ã«çµåˆã—ã¦ã²ã¨ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦ã‚¯ã‚¨ãƒªã™ã‚‹ã€‚LLMã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’è¶…ãˆã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹
    # ResponseMode.GENERATION	ãƒãƒ¼ãƒ‰æƒ…å ±ã‚’ä½¿ã‚ãšå˜ã«LLMã«å•ã„åˆã‚ã›ã‚‹
    # ResponseMode.ACCUMULATE	å„ãƒãƒ¼ãƒ‰ã«å¯¾ã™ã‚‹LLMã®çµæœã‚’è¨ˆç®—ã—ã¦çµæœã‚’é€£çµã™ã‚‹
    # ResponseMode.COMPACT_ACCUMULATE	ACCUMULATEã¨ã»ã¼åŒã˜ã ãŒã€ãƒãƒ£ãƒ³ã‚¯ã‚’LLMã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æœ€å¤§é™åˆ©ç”¨ã™ã‚‹ã‚ˆã†ã«repackã™ã‚‹ãŸã‚ã‚ˆã‚Šé«˜é€Ÿ
    response_mode=ResponseMode.COMPACT
)

chat_memory = ChatMemoryBuffer.from_defaults(
    token_limit=10000,
)

query_engine = RetrieverQueryEngine(
    retriever=retriever,
    response_synthesizer=response_synthesizer,
)

# ãƒãƒ£ãƒƒãƒˆå®Ÿè¡Œ
while True:
    user_input = input("ğŸ‘¤ ã‚ãªãŸ: ")
    if user_input.lower() in ["exit", "quit"]:
        break
    response = query_engine.query(user_input)
    print(chat_memory.get_all())
    print(f"ğŸ¤– AI: {response.response}")
